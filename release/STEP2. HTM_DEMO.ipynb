{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "from streamReader import streamReader\n",
    "from htm_anomaly_detection import HTM\n",
    "from nupic.engine import Network\n",
    "from nupic.encoders import MultiEncoder, ScalarEncoder, DateEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before run the code below, make sure you're running \"step1. data simulator.ipynb\" to generate the data cache\n",
    "### Example1. use it without loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "parameters that we need to define\n",
    "'''\n",
    "_TIMEOFDAY = (21,4)  # timeofday = internal buffer, see this: https://nupic.docs.numenta.org/1.0.3/api/algorithms/encoders.html\n",
    "_SAVEMODEL = False\n",
    "_USE_SAVED_MODEL = False\n",
    "\n",
    "\n",
    "'''\n",
    "Initialize HTM anomaly detection object\n",
    "'''\n",
    "htm = HTM(use_saved_model = _USE_SAVED_MODEL)\n",
    "\n",
    "\n",
    "'''\n",
    "Run the code below once to set encoders, SP, and TM parameters. \n",
    "After this, if you don't want to change these parameters then you should just read them from files.\n",
    "'''\n",
    "encoder_params = {\n",
    "   'scalarEncoder1Args':{\n",
    "     \"minval\": 0,\n",
    "     \"maxval\": 20,\n",
    "     \"w\": 21,\n",
    "     \"periodic\": False,\n",
    "     \"n\": 50,\n",
    "     \"radius\": 0,\n",
    "     \"resolution\": 0,\n",
    "     \"name\": \"value\",\n",
    "     \"verbosity\": 0,\n",
    "     \"clipInput\": False,\n",
    "     \"forced\": False,\n",
    "   },\n",
    "       \n",
    "   'dateEncoder1Args':{\n",
    "     \"season\": 0,\n",
    "     \"dayOfWeek\": 0,\n",
    "     \"weekend\": 0,\n",
    "     \"holiday\": 0,\n",
    "     \"timeOfDay\": _TIMEOFDAY,\n",
    "     \"customDays\": 0,\n",
    "     \"name\": \"time\",\n",
    "     \"forced\": False\n",
    "   },\n",
    "       \n",
    "   'scalarEncoder2Args':{\n",
    "     \"minval\": 0,\n",
    "     \"maxval\": 20,\n",
    "     \"w\": 21,\n",
    "     \"periodic\": False,\n",
    "     \"n\": 50,\n",
    "     \"radius\": 0,\n",
    "     \"resolution\": 0,\n",
    "     \"name\": \"value\",\n",
    "     \"verbosity\": 0,\n",
    "     \"clipInput\": False,\n",
    "     \"forced\": False,\n",
    "   },\n",
    "   \n",
    "   'dateEncoder2Args':{\n",
    "     \"season\": 0,\n",
    "     \"dayOfWeek\": 0,\n",
    "     \"weekend\": 0,\n",
    "     \"holiday\": 0,\n",
    "     \"timeOfDay\": _TIMEOFDAY,\n",
    "     \"customDays\": 0,\n",
    "     \"name\": \"time\",\n",
    "     \"forced\": False\n",
    "   }\n",
    "}\n",
    "       \n",
    "htm.setEncoderParams('./encoders.json', encoder_params)\n",
    "\n",
    "_SP_PARAMS = {\n",
    "   'SP':{\n",
    "       \"spatialImp\": \"cpp\",\n",
    "       \"globalInhibition\": 1,\n",
    "       \"columnCount\": 2048,\n",
    "       \"inputWidth\": 0,\n",
    "       \"numActiveColumnsPerInhArea\": 40,\n",
    "       \"seed\": 1956,\n",
    "       \"potentialPct\": 0.8,\n",
    "       \"synPermConnected\": 0.1,\n",
    "       \"synPermActiveInc\": 0.0001,\n",
    "       \"synPermInactiveDec\": 0.0005,\n",
    "       \"boostStrength\": 0.0,\n",
    "   }\n",
    "}\n",
    "\n",
    "_TM_PARAMS = {\n",
    "   'TM':{\n",
    "       \"columnCount\": 2048,\n",
    "       \"cellsPerColumn\": 32,\n",
    "       \"inputWidth\": 2048,\n",
    "       \"seed\": 1960,\n",
    "       \"temporalImp\": \"cpp\",\n",
    "       \"newSynapseCount\": 20,\n",
    "       \"maxSynapsesPerSegment\": 32,\n",
    "       \"maxSegmentsPerCell\": 128,\n",
    "       \"initialPerm\": 0.21,\n",
    "       \"permanenceInc\": 0.1,\n",
    "       \"permanenceDec\": 0.1,\n",
    "       \"globalDecay\": 0.0,\n",
    "       \"maxAge\": 0,\n",
    "       \"minThreshold\": 9,\n",
    "       \"activationThreshold\": 12,\n",
    "       \"outputType\": \"normal\",\n",
    "       \"pamLength\": 3,\n",
    "   }\n",
    "}\n",
    "\n",
    "htm.setEncoderParams('./SP.json', _SP_PARAMS)\n",
    "htm.setEncoderParams('./TM.json', _TM_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load the above parameters from json files and build data stream reader to read data from cache.\n",
    "The data has to be in the exact format of cache generated by \"Step1. data simulator.ipynb\"\n",
    "'''\n",
    "\n",
    "scalarEncoder1Args = htm.getEncoderParams('./encoders.json', 'scalarEncoder1Args')\n",
    "dateEncoder1Args = htm.getEncoderParams('./encoders.json', 'dateEncoder1Args')\n",
    "scalarEncoder2Args = htm.getEncoderParams('./encoders.json', 'scalarEncoder2Args')\n",
    "dateEncoder2Args = htm.getEncoderParams('./encoders.json', 'dateEncoder2Args')\n",
    "SPArgs = htm.getEncoderParams('./SP.json', 'SP')\n",
    "TMArgs = htm.getEncoderParams('./TM.json', 'TM')\n",
    "\n",
    "input01_recordParams = {\n",
    "  \"scalarEncoderArgs\": scalarEncoder1Args,\n",
    "  \"dateEncoderArgs\": dateEncoder1Args,\n",
    "}\n",
    "\n",
    "input02_recordParams = {\n",
    "  \"scalarEncoderArgs\": scalarEncoder2Args,\n",
    "  \"dateEncoderArgs\": dateEncoder2Args,\n",
    "}\n",
    "\n",
    "# define the data format in cache\n",
    "names = ['time', 'value']\n",
    "types = ['datetime', 'float']\n",
    "specials = ['T', '']\n",
    "streamReader1 = streamReader(streamID = './datacache01', names = names, types = types, specials = specials)\n",
    "streamReader2 = streamReader(streamID = './datacache02', names = names, types = types, specials = specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create networks and tell them the source of data for prediction\n",
    "'''\n",
    "network01 = htm.createNetwork(datasource=streamReader1, recordParams=input01_recordParams, spatialParams=SPArgs, temporalParams=TMArgs)\n",
    "network02 = htm.createNetwork(datasource=streamReader2, recordParams=input02_recordParams, spatialParams=SPArgs, temporalParams=TMArgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fed_in_data01 11.4441  anomaly likelihood: 0.5\n",
      "fed_in_data01 10.9207  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 13.4668  anomaly likelihood: 0.5\n",
      "fed_in_data01 2.66524  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 4.23665  anomaly likelihood: 0.5\n",
      "fed_in_data01 11.1507  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 14.4405  anomaly likelihood: 0.5\n",
      "fed_in_data01 2.91072  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 2.85113  anomaly likelihood: 0.5\n",
      "fed_in_data01 6.16676  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 14.8674  anomaly likelihood: 0.5\n",
      "fed_in_data01 12.1911  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 7.50275  anomaly likelihood: 0.5\n",
      "fed_in_data01 3.52322  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 11.9118  anomaly likelihood: 0.5\n",
      "fed_in_data01 9.50803  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 4.49895  anomaly likelihood: 0.5\n",
      "fed_in_data01 0.191487  anomaly likelihood: 0.5 \n",
      "\n",
      "fed_in_data01 7.61852  anomaly likelihood: 0.5\n",
      "fed_in_data01 10.9783  anomaly likelihood: 0.5 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Looping network.run() to get iterative prediction from data cache.\n",
    "The generator will keep writing to data cache so while(1) loop will not touch to the end of bottom.\n",
    "I use 10 iteration here to show you how to save the model.\n",
    "'''\n",
    "iteration = 0\n",
    "while(iteration < 10):\n",
    "    fed_in_data01, anomalyLikelihood1 = htm.run(network01)\n",
    "    fed_in_data02, anomalyLikelihood2 = htm.run(network02)\n",
    "    print 'fed_in_data01',fed_in_data01,' anomaly likelihood:',anomalyLikelihood1\n",
    "    print 'fed_in_data01',fed_in_data02,' anomaly likelihood:',anomalyLikelihood2, '\\n'\n",
    "    iteration += 1\n",
    "    time.sleep(2)\n",
    "htm.save_network(network01, './models/network1.nta')\n",
    "htm.save_network(network02, './models/network2.nta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example2. use it by loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComming soon...\\nFor the current data stream reader, there are still some problems associated with this part.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Comming soon...\n",
    "For the current data stream reader, there are still some problems associated with this part.\n",
    "'''\n",
    "\n",
    "# _USE_SAVED_MODEL = True\n",
    "# htm = HTM(use_saved_model = _USE_SAVED_MODEL)\n",
    "\n",
    "# scalarEncoder1Args = htm.getEncoderParams('./encoders.json', 'scalarEncoder1Args')\n",
    "# dateEncoder1Args = htm.getEncoderParams('./encoders.json', 'dateEncoder1Args')\n",
    "# scalarEncoder2Args = htm.getEncoderParams('./encoders.json', 'scalarEncoder2Args')\n",
    "# dateEncoder2Args = htm.getEncoderParams('./encoders.json', 'dateEncoder2Args')\n",
    "# SPArgs = htm.getEncoderParams('./SP.json', 'SP')\n",
    "# TMArgs = htm.getEncoderParams('./TM.json', 'TM')\n",
    "\n",
    "# input01_recordParams = {\n",
    "#   \"scalarEncoderArgs\": scalarEncoder1Args,\n",
    "#   \"dateEncoderArgs\": dateEncoder1Args,\n",
    "# }\n",
    "\n",
    "# input02_recordParams = {\n",
    "#   \"scalarEncoderArgs\": scalarEncoder2Args,\n",
    "#   \"dateEncoderArgs\": dateEncoder2Args,\n",
    "# }\n",
    "\n",
    "# # names = ['time', 'value']\n",
    "# # types = ['datetime', 'float']\n",
    "# # specials = ['T', '']\n",
    "# # streamReader1 = streamReader(streamID = './datacache01', names = names, types = types, specials = specials)\n",
    "# # streamReader2 = streamReader(streamID = './datacache02', names = names, types = types, specials = specials)\n",
    "\n",
    "# model1 = './models/network1.nta'\n",
    "# model2 = './models/network2.nta'\n",
    "# network01 = htm.createNetwork(datasource=None, recordParams=input01_recordParams, spatialParams=None, temporalParams=None, model_path = model1)\n",
    "# network02 = htm.createNetwork(datasource=None, recordParams=input02_recordParams, spatialParams=None, temporalParams=None, model_path = model2)\n",
    "\n",
    "# iteration = 0\n",
    "# while(iteration < 5):\n",
    "#     fed_in_data01, anomalyLikelihood1 = htm.run(network01)\n",
    "#     fed_in_data02, anomalyLikelihood2 = htm.run(network02)\n",
    "#     print 'fed_in_data01',fed_in_data01,' anomaly likelihood:',anomalyLikelihood1\n",
    "#     print 'fed_in_data01',fed_in_data02,' anomaly likelihood:',anomalyLikelihood2, '\\n'\n",
    "#     iteration += 1\n",
    "#     time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
